##########################################################################
# MULTIMNO - BASE
##########################################################################

FROM ubuntu:22.04 as data-eng-base

# # FROM data-eng-base as data-eng-dev

# # continuumio/miniconda3:master

# #FROM continuumio/miniconda3:23.5.2-0-alpine as data-eng-base

# WORKDIR /opt/dev/

# # FROM continuumio/miniconda3:23.10.0-1 as data-eng-base
# # docker pull continuumio/miniconda3:master

# # docker pull continuumio/miniconda3:23.10.0-1

# WORKDIR /opt/dev/
# ARG install_dir=/tmp/install

# # RUN conda env create -f requirements/environment.yml
# # RUN /bin/bash -c "conda env create -f requirements/environment.yml"

# # --------- Set tzdata ----------
# # Set the timezone
# ENV TZ=Europe/Tallinn
# RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone
# RUN apt-get update && apt-get install -y tzdata

# # Default shell
# # ENV SHELL /bin/bash

# # --------- INSTALL Python --------
# RUN apt update && \
#   apt install -y software-properties-common curl

# RUN add-apt-repository ppa:deadsnakes/ppa
# # RUN apt-get update 
# # RUN apt install -y python3.11-dev

# # RUN yes | apt install python3.9-distutils

# #RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1
# #RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# #RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.11 1
# #RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1

# # Install pip
# #RUN curl https://bootstrap.pypa.io/get-pip.py | python

# # ---------- INSTALL System Libraries ----------

# RUN apt-get update && \
#   apt-get install -y --no-install-recommends \
#   sudo \
#   build-essential \
#   software-properties-common \
#   openssh-client openssh-server \
#   ssh \
#   wget

# # ---------- SPARK ----------
# # Setup the directories for Spark/Hadoop installation
# # ENV SPARK_HOME=${SPARK_HOME:-"/opt/spark"}

# # ENV SPARK_HOME=${SPARK_HOME:-"/opt/spark"}

# # Create spark folder
# # RUN mkdir -p ${SPARK_HOME}
# WORKDIR /opt/dev/


# RUN mkdir -p /opt/conda 
# RUN wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.12.0-Linux-x86_64.sh -O /opt/conda/miniconda.sh \ && bash /opt/conda/miniconda.sh -b -p /opt/miniconda 

# CONDA
# RUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /opt/miniconda.sh && /bin/bash /opt/miniconda.sh -b -p /opt/conda

# # RUN wget --quiet \
# #     https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
# #     && mkdir /root/.conda \
# #     && bash Miniconda3-latest-Linux-x86_64.sh -b \
# #     && rm -f Miniconda3-latest-Linux-x86_64.sh 
# RUN conda --version

FROM python:slim
RUN apt-get update && apt-get -y upgrade \
  && apt-get install -y --no-install-recommends \
    wget \
    g++ \
    gcc \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*


ENV PATH="/root/miniconda3/bin:${PATH}"
ARG PATH="/root/miniconda3/bin:${PATH}"
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \
    && mkdir /root/.conda \
    && bash Miniconda3-latest-Linux-x86_64.sh -b \
    && rm -f Miniconda3-latest-Linux-x86_64.sh \
    && echo "Running $(conda --version)" 



COPY requirements/environment.yml ${install_dir}/requirements/environment.yml
#COPY requirements/requirements.txt ${install_dir}/requirements/requirements.txt
RUN conda env create -f ${install_dir}/requirements/environment.yml
#RUN pip install -r ${install_dir}/requirements/requirements.txt





# ARG SPARK_VERSION
# Download and install Spark
# RUN wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz \
#   && tar -xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \
#   && rm -rf spark-${SPARK_VERSION}-bin-hadoop3.tgz

# ENV SPARK_VERSION=${SPARK_VERSION}

# ---------- PYTHON DEPENDENCIES ----------
# ---------- INSTALL poetry / pip-tools ----------
# NEEDED?
# RUN pip3 install poetry pip-tools

# # Install requirements
# ARG install_dir=/tmp/install

# # Upgrade pip to latest version
# RUN pip install --upgrade pip


# Conda install and env def

#RUN curl https://repo.anaconda.com/miniconda/Miniconda3-py310_23.5.2-0-Linux-x86_64.sh --output miniconda.sh
#RUN bash miniconda.sh | yes
#RUN rm miniconda.sh

# improve path management

#RUN python -m venv venv
#RUN . /venv/bin/activate
#RUN python -m pip install --upgrade pip

#RUN python -m pip install -r requirements/requirements.txt
#RUN conda env create --file=environment.yml
#RUN conda deactivate
#RUN conda activate scraper_env
#RUN conda deactivate

# Standard requirements
# COPY requirements/requirements.in ${install_dir}/requirements/requirements.in
# #COPY requirements/requirements.txt ${install_dir}/requirements/requirements.txt
# RUN pip-compile ${install_dir}/requirements/requirements.in 
# #RUN pip install -r ${install_dir}/requirements/requirements.txt


# Dev requirements
# COPY requirements/dev_requirements.in ${install_dir}/requirements/dev_requirements.in
# RUN pip-compile ${install_dir}/requirements/dev_requirements.in
# RUN pip install -r ${install_dir}/requirements/dev_requirements.txt

# Setup airflow

# # Add jupyterlab alias
# RUN echo "alias jl='jupyter lab --ip=0.0.0.0 --port=8888 --no-browser  \
#   --allow-root --NotebookApp.base_url=${JUPYTER_BASE_URL} --NotebookApp.token='" >> ~/.bashrc

# Set Path environment variable
# ENV PATH="${PATH}:$SPARK_HOME/bin:$SPARK_HOME/sbin"

# ----------- CLEANUP -----------
RUN rm -r ${install_dir}
RUN rm -rf /var/lib/apt/lists/*


##########################################################################
# MULTIMNO - DEV
##########################################################################
# FROM data-eng-base as data-eng-dev

# ---------- SEDONA ----------
# Args
# ARG SCALA_VERSION
# ARG SEDONA_VERSION
# ARG GEOTOOLS_WRAPPER_VERSION


# ENV SCALA_VERSION=${SCALA_VERSION}
# ENV SEDONA_VERSION=${SEDONA_VERSION}
# ENV GEOTOOLS_WRAPPER_VERSION=${GEOTOOLS_WRAPPER_VERSION}

# Install sedona jars
# COPY scripts/install_sedona_jars.sh ${install_dir}/scripts/install_sedona_jars.sh
# RUN ${install_dir}/scripts/install_sedona_jars.sh ${SPARK_VERSION} ${SCALA_VERSION} ${SEDONA_VERSION} ${GEOTOOLS_WRAPPER_VERSION} 
# ------------------------------

# Install git
# RUN apt update && apt install -y git 

# ----------- CLEANUP -----------
# RUN rm -r ${install_dir}
# RUN rm -rf /var/lib/apt/lists/*

# # ----------- RUNTIME -----------
# # Copy the default configurations into $SPARK_HOME/conf
# COPY conf/spark-defaults.conf "$SPARK_HOME/conf/spark-defaults.conf"
# COPY conf/log4j2.properties "$SPARK_HOME/conf/log4j2.properties"

# ENV PYTHONPATH=${SPARK_HOME}/python:/opt/dev
#ENV SPARK_HOME=${SPARK_HOME:-"/opt/spark"}

ENV PYTHONPATH=/opt/dev
# WORKDIR /opt/dev

EXPOSE 3000
EXPOSE 4040

CMD ["bash"]